{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842e34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import cv2\n",
    "import yaml\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "154bff2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 183 images belonging to 2 classes.\n",
      "Found 47 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "61/61 [==============================] - 22s 364ms/step - loss: 1.1818 - accuracy: 0.6831 - val_loss: 0.6804 - val_accuracy: 0.6809\n",
      "Epoch 2/30\n",
      "61/61 [==============================] - 23s 371ms/step - loss: 0.4845 - accuracy: 0.7486 - val_loss: 0.7260 - val_accuracy: 0.6809\n",
      "Epoch 3/30\n",
      "61/61 [==============================] - 23s 380ms/step - loss: 0.3898 - accuracy: 0.8361 - val_loss: 0.6830 - val_accuracy: 0.6809\n",
      "Epoch 4/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.3912 - accuracy: 0.8852\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "61/61 [==============================] - 23s 375ms/step - loss: 0.3912 - accuracy: 0.8852 - val_loss: 0.5566 - val_accuracy: 0.6809\n",
      "Epoch 5/30\n",
      "61/61 [==============================] - 23s 379ms/step - loss: 0.2598 - accuracy: 0.9071 - val_loss: 0.9176 - val_accuracy: 0.6809\n",
      "Epoch 6/30\n",
      "61/61 [==============================] - 23s 373ms/step - loss: 0.2208 - accuracy: 0.9126 - val_loss: 0.5123 - val_accuracy: 0.7234\n",
      "Epoch 7/30\n",
      "61/61 [==============================] - 24s 389ms/step - loss: 0.1804 - accuracy: 0.9399 - val_loss: 0.4976 - val_accuracy: 0.8511\n",
      "Epoch 8/30\n",
      "61/61 [==============================] - 23s 381ms/step - loss: 0.0948 - accuracy: 0.9617 - val_loss: 0.5550 - val_accuracy: 0.8511\n",
      "Epoch 9/30\n",
      "61/61 [==============================] - 23s 380ms/step - loss: 0.1159 - accuracy: 0.9399 - val_loss: 0.4930 - val_accuracy: 0.8298\n",
      "Epoch 10/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9781\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "61/61 [==============================] - 23s 371ms/step - loss: 0.0856 - accuracy: 0.9781 - val_loss: 0.6574 - val_accuracy: 0.8298\n",
      "Epoch 11/30\n",
      "61/61 [==============================] - 23s 377ms/step - loss: 0.0463 - accuracy: 0.9836 - val_loss: 0.7800 - val_accuracy: 0.7660\n",
      "Epoch 12/30\n",
      "61/61 [==============================] - 25s 403ms/step - loss: 0.0503 - accuracy: 0.9891 - val_loss: 0.6275 - val_accuracy: 0.8723\n",
      "Epoch 13/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9891\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "61/61 [==============================] - 25s 407ms/step - loss: 0.0307 - accuracy: 0.9891 - val_loss: 0.6467 - val_accuracy: 0.8723\n",
      "Epoch 14/30\n",
      "61/61 [==============================] - 24s 391ms/step - loss: 0.0251 - accuracy: 0.9891 - val_loss: 0.5747 - val_accuracy: 0.8723\n",
      "Epoch 15/30\n",
      "61/61 [==============================] - 26s 424ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.6247 - val_accuracy: 0.8723\n",
      "Epoch 16/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "61/61 [==============================] - 25s 408ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.6651 - val_accuracy: 0.8723\n",
      "Epoch 17/30\n",
      "61/61 [==============================] - 24s 390ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.6490 - val_accuracy: 0.8723\n",
      "Epoch 18/30\n",
      "61/61 [==============================] - 23s 375ms/step - loss: 0.0196 - accuracy: 0.9945 - val_loss: 0.6445 - val_accuracy: 0.8511\n",
      "Epoch 19/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "61/61 [==============================] - 23s 373ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.6478 - val_accuracy: 0.8511\n",
      "Epoch 20/30\n",
      "61/61 [==============================] - 23s 379ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 0.6314 - val_accuracy: 0.8723\n",
      "Epoch 21/30\n",
      "61/61 [==============================] - 23s 374ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 0.8723\n",
      "Epoch 22/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "61/61 [==============================] - 23s 374ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 0.8723\n",
      "Epoch 23/30\n",
      "61/61 [==============================] - 23s 371ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.6429 - val_accuracy: 0.8723\n",
      "Epoch 24/30\n",
      "61/61 [==============================] - 23s 371ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.6448 - val_accuracy: 0.8723\n",
      "Epoch 25/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "61/61 [==============================] - 22s 367ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.6526 - val_accuracy: 0.8723\n",
      "Epoch 26/30\n",
      "61/61 [==============================] - 22s 360ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.6536 - val_accuracy: 0.8723\n",
      "Epoch 27/30\n",
      "61/61 [==============================] - 22s 365ms/step - loss: 0.0149 - accuracy: 0.9945 - val_loss: 0.6589 - val_accuracy: 0.8723\n",
      "Epoch 28/30\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "61/61 [==============================] - 22s 366ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.6544 - val_accuracy: 0.8723\n",
      "Epoch 29/30\n",
      "61/61 [==============================] - 23s 377ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.6549 - val_accuracy: 0.8723\n",
      "Epoch 30/30\n",
      "61/61 [==============================] - 23s 381ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.6551 - val_accuracy: 0.8723\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    " \n",
    "''' 搭建模型'''\n",
    "l=tf.keras.layers\n",
    "model=Sequential()\n",
    " \n",
    "#第一层卷积和池化\n",
    "model.add(l.Conv2D(filters=96,kernel_size=(11,11),strides=(4,4),padding='valid',input_shape=(227,227,3),activation='relu'))\n",
    "model.add(l.BatchNormalization())\n",
    "model.add(l.MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='valid'))\n",
    " \n",
    "#第二层卷积和池化\n",
    "model.add(l.Conv2D(256,(5,5),(1,1),padding='same',activation='relu'))\n",
    "model.add(l.BatchNormalization())\n",
    "model.add(l.MaxPooling2D((3,3),(2,2),padding='valid'))\n",
    " \n",
    "#第三层卷积\n",
    "model.add(l.Conv2D(384,(3,3),(1,1),'same',activation='relu'))\n",
    " \n",
    "#第四层卷积\n",
    "model.add(l.Conv2D(384,(3,3),(1,1),'same',activation='relu'))\n",
    " \n",
    "#第五层卷积和池化\n",
    "model.add(l.Conv2D(256,(3,3),(1,1),'same',activation='relu'))\n",
    "model.add(l.MaxPooling2D((3,3),(2,2),'valid'))\n",
    " \n",
    "#全连接层\n",
    "model.add(l.Flatten())\n",
    "model.add(l.Dense(4096,activation='relu'))\n",
    "model.add(l.Dropout(0.5))\n",
    " \n",
    "model.add(l.Dense(4096,activation='relu'))\n",
    "model.add(l.Dropout(0.5))\n",
    " \n",
    "model.add(l.Dense(1000,activation='relu'))\n",
    "model.add(l.Dropout(0.5))\n",
    " \n",
    "#输出层\n",
    "model.add(l.Dense(2,activation='softmax'))\n",
    "model.compile(optimizer='adam', loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    " \n",
    "'''导入图片数据'''\n",
    "\n",
    "# ZipFile(\"/home/dzj/caofeng/image/image_train.zip\",\"r\").extractall(\"/home/dzj/caofeng/image/\")\n",
    "# ZipFile(\"/home/dzj/caofeng/image/image_test1.zip\", \"r\").extractall(\"/home/dzj/caofeng/image/\")\n",
    "\n",
    "# image_train = \"/home/dzj/caofeng/image/image_train\"\n",
    "# image_test = \"/home/dzj/caofeng/image/image_test1\"\n",
    "\n",
    "#利用ImageDataGenerator生成一个batch一个batch的数据\n",
    "datagen=ImageDataGenerator(samplewise_center=True,rescale=1.0/255)   #samplewise_center:使输入数据的每个样本均值为0,rescale:归一化\n",
    " \n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'D:/image/dataset/image_train',\n",
    "    classes=['hg','bhg'],\n",
    "    target_size=(227, 227),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    'D:/image/dataset/image_test1',\n",
    "    classes=['hg','bhg'],\n",
    "    target_size=(227, 227),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "checkpoint_filepath = 'AlexNet_model2.h5'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "callback_learningrate = ReduceLROnPlateau(monitor='val_accuracy', mode='max', min_delta=0.03, patience=3, factor=.5, min_lr=0.00001, verbose=1)\n",
    "callbacks = [callback_checkpoint, callback_learningrate]\n",
    "\n",
    "'''开始训练'''\n",
    "model.fit_generator(generator=train_generator,steps_per_epoch=183/3,epochs=30,validation_data=validation_generator,validation_steps=47/3,callbacks=callbacks)\n",
    "\n",
    "yaml_string = model.to_yaml()  # 保存模型结构到yaml文件\n",
    "open('./model_architecture2.yaml', 'w').write(yaml_string)\n",
    "model.save_weights('./AlexNet_model2.h5')  #保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cf7310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''导入模型'''\n",
    "model = model_from_yaml(open('./model_architecture.yaml').read())\n",
    "model.load_weights('./AlexNet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import cv2\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def url_to_image(url):\n",
    "    # download the image, convert it to a NumPy array, and then read\n",
    "    # it into OpenCV format\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    # bytearray将数据转换成（返回）一个新的字节数组\n",
    "    # asarray 复制数据，将结构化数据转换成ndarray\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    # cv2.imdecode()函数将数据解码成Opencv图像格式\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    # return the image\n",
    "    return image\n",
    "imgs=[]\n",
    "image = url_to_image('https://dpic.tiankong.com/t1/nl/QJ6989364695.jpg?x-oss-process=style/794ws')\n",
    "# plt.imshow(image)\n",
    "img = cv2.resize(image,(227,227))\n",
    "imgs.append(img)\n",
    "a = np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1378ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''导入模型'''\n",
    "model = model_from_yaml(open('./model_architecture.yaml').read())\n",
    "model.load_weights('./AlexNet_model.h5')\n",
    " \n",
    "'''随便输入一张图片测试一下'''\n",
    "imgs=[]\n",
    "img=cv2.imread('D:/image/7d5f69b950ab832c639f03baa659de7.png')\n",
    "img=cv2.resize(img,(227,227))\n",
    "imgs.append(img)\n",
    "a=np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc8a910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is bhg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=model.predict(a)\n",
    "idx=np.argmax(result)\n",
    " \n",
    "if idx==0:\n",
    "    print('the image is hg\\n')\n",
    "else:\n",
    "    print('the image is bhg\\n')\n",
    "\n",
    "# cv2.imshow(\"image\",img)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af109cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''随便输入一张图片测试一下'''\n",
    "imgs=[]\n",
    "img=cv2.imread('D:/image/16.jpg')\n",
    "img=cv2.resize(img,(227,227))\n",
    "imgs.append(img)\n",
    "a=np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de31b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_image(url):\n",
    "    # download the image, convert it to a NumPy array, and then read\n",
    "    # it into OpenCV format\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    # bytearray将数据转换成（返回）一个新的字节数组\n",
    "    # asarray 复制数据，将结构化数据转换成ndarray\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    # cv2.imdecode()函数将数据解码成Opencv图像格式\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    # return the image\n",
    "    return image\n",
    "image = url_to_image('https://dzj-prod-1.oss-cn-shanghai.aliyuncs.com/2021/03/08/60459f23e4b0783a977e073f.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc4aa6b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = url_to_image('https://dzj-prod-1.oss-cn-shanghai.aliyuncs.com/2021/03/08/60459f23e4b0783a977e073f.png')\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
