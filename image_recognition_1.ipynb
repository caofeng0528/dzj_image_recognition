{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d490487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import cv2\n",
    "import yaml\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6bff1db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210 images belonging to 2 classes.\n",
      "Found 47 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "42/42 [==============================] - 27s 639ms/step - loss: 1.3155 - accuracy: 0.5429 - val_loss: 0.6576 - val_accuracy: 0.6809\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - 27s 637ms/step - loss: 0.7172 - accuracy: 0.5714 - val_loss: 0.6424 - val_accuracy: 0.6809\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 26s 630ms/step - loss: 0.6917 - accuracy: 0.5952 - val_loss: 0.6628 - val_accuracy: 0.6809\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.6667 - accuracy: 0.5905\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0029999999329447745.\n",
      "42/42 [==============================] - 26s 631ms/step - loss: 0.6667 - accuracy: 0.5905 - val_loss: 0.6580 - val_accuracy: 0.6170\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 26s 626ms/step - loss: 0.5877 - accuracy: 0.6810 - val_loss: 0.6316 - val_accuracy: 0.6383\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 26s 617ms/step - loss: 0.6125 - accuracy: 0.6619 - val_loss: 0.6202 - val_accuracy: 0.6170\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.5757 - accuracy: 0.6857\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009000000078231095.\n",
      "42/42 [==============================] - 26s 630ms/step - loss: 0.5757 - accuracy: 0.6857 - val_loss: 0.5672 - val_accuracy: 0.7021\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 26s 630ms/step - loss: 0.5547 - accuracy: 0.6810 - val_loss: 0.5319 - val_accuracy: 0.7872\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 27s 634ms/step - loss: 0.4941 - accuracy: 0.7476 - val_loss: 0.5193 - val_accuracy: 0.7447\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 26s 630ms/step - loss: 0.5370 - accuracy: 0.6810 - val_loss: 0.4953 - val_accuracy: 0.7660\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.7381\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00026999999536201356.\n",
      "42/42 [==============================] - 27s 632ms/step - loss: 0.4971 - accuracy: 0.7381 - val_loss: 0.4812 - val_accuracy: 0.7021\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 27s 637ms/step - loss: 0.4587 - accuracy: 0.8190 - val_loss: 0.4284 - val_accuracy: 0.8511\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 26s 629ms/step - loss: 0.4936 - accuracy: 0.7905 - val_loss: 0.4530 - val_accuracy: 0.8085\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 26s 626ms/step - loss: 0.4723 - accuracy: 0.7857 - val_loss: 0.4348 - val_accuracy: 0.8085\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4788 - accuracy: 0.7762\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.099999686237424e-05.\n",
      "42/42 [==============================] - 27s 637ms/step - loss: 0.4788 - accuracy: 0.7762 - val_loss: 0.4660 - val_accuracy: 0.7447\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 26s 617ms/step - loss: 0.4408 - accuracy: 0.8429 - val_loss: 0.4251 - val_accuracy: 0.7660\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 26s 627ms/step - loss: 0.4610 - accuracy: 0.7762 - val_loss: 0.4137 - val_accuracy: 0.7660\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.7905\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-05.\n",
      "42/42 [==============================] - 26s 617ms/step - loss: 0.4613 - accuracy: 0.7905 - val_loss: 0.4531 - val_accuracy: 0.7660\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 26s 615ms/step - loss: 0.4582 - accuracy: 0.7762 - val_loss: 0.4240 - val_accuracy: 0.8085\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 26s 624ms/step - loss: 0.4424 - accuracy: 0.7714 - val_loss: 0.4528 - val_accuracy: 0.7872\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4691 - accuracy: 0.7905\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-06.\n",
      "42/42 [==============================] - 26s 624ms/step - loss: 0.4691 - accuracy: 0.7905 - val_loss: 0.4756 - val_accuracy: 0.7660\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 26s 622ms/step - loss: 0.4926 - accuracy: 0.7667 - val_loss: 0.4801 - val_accuracy: 0.7447\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 26s 622ms/step - loss: 0.4852 - accuracy: 0.7571 - val_loss: 0.4872 - val_accuracy: 0.7234\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4744 - accuracy: 0.7714\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 2.186999927289435e-06.\n",
      "42/42 [==============================] - 26s 623ms/step - loss: 0.4744 - accuracy: 0.7714 - val_loss: 0.4049 - val_accuracy: 0.8085\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 26s 622ms/step - loss: 0.4474 - accuracy: 0.7810 - val_loss: 0.4919 - val_accuracy: 0.7234\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 26s 622ms/step - loss: 0.4809 - accuracy: 0.7762 - val_loss: 0.4884 - val_accuracy: 0.7021\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4456 - accuracy: 0.7571\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.560999509019894e-07.\n",
      "42/42 [==============================] - 26s 619ms/step - loss: 0.4456 - accuracy: 0.7571 - val_loss: 0.4801 - val_accuracy: 0.7660\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 26s 617ms/step - loss: 0.4148 - accuracy: 0.8048 - val_loss: 0.4449 - val_accuracy: 0.7234\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 26s 618ms/step - loss: 0.4290 - accuracy: 0.7952 - val_loss: 0.4802 - val_accuracy: 0.7660\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4389 - accuracy: 0.7667\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.9682997844938655e-07.\n",
      "42/42 [==============================] - 26s 630ms/step - loss: 0.4389 - accuracy: 0.7667 - val_loss: 0.4899 - val_accuracy: 0.7660\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    " \n",
    "''' 搭建模型'''\n",
    "l=tf.keras.layers\n",
    "model=Sequential()\n",
    " \n",
    "#第一层卷积和池化\n",
    "model.add(l.Conv2D(filters=96,kernel_size=(11,11),strides=(4,4),padding='valid',input_shape=(227,227,3),activation='relu'))\n",
    "model.add(l.BatchNormalization())\n",
    "model.add(l.MaxPooling2D(pool_size=(3,3),strides=(2,2),padding='valid'))\n",
    " \n",
    "#第二层卷积和池化\n",
    "model.add(l.Conv2D(256,(5,5),(1,1),padding='same',activation='relu'))\n",
    "model.add(l.BatchNormalization())\n",
    "model.add(l.MaxPooling2D((3,3),(2,2),padding='valid'))\n",
    " \n",
    "#第三层卷积\n",
    "model.add(l.Conv2D(384,(3,3),(1,1),'same',activation='relu'))\n",
    " \n",
    "#第四层卷积\n",
    "model.add(l.Conv2D(384,(3,3),(1,1),'same',activation='relu'))\n",
    " \n",
    "#第五层卷积和池化\n",
    "model.add(l.Conv2D(256,(3,3),(1,1),'same',activation='relu'))\n",
    "model.add(l.MaxPooling2D((3,3),(2,2),'valid'))\n",
    " \n",
    "#全连接层\n",
    "model.add(l.Flatten())\n",
    "model.add(l.Dense(4096,activation='relu'))\n",
    "model.add(l.Dropout(0.5))\n",
    " \n",
    "model.add(l.Dense(4096,activation='relu'))\n",
    "model.add(l.Dropout(0.5))\n",
    " \n",
    "model.add(l.Dense(1000,activation='relu'))\n",
    "model.add(l.Dropout(0.5))\n",
    " \n",
    "#输出层\n",
    "model.add(l.Dense(2,activation='softmax'))\n",
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "'''导入图片数据'''\n",
    "\n",
    "# ZipFile(\"/home/dzj/caofeng/image/image_train.zip\",\"r\").extractall(\"/home/dzj/caofeng/image/\")\n",
    "# ZipFile(\"/home/dzj/caofeng/image/image_test1.zip\", \"r\").extractall(\"/home/dzj/caofeng/image/\")\n",
    "\n",
    "# image_train = \"/home/dzj/caofeng/image/image_train\"\n",
    "# image_test = \"/home/dzj/caofeng/image/image_test1\"\n",
    "\n",
    "#利用ImageDataGenerator生成一个batch一个batch的数据\n",
    "datagen=ImageDataGenerator(samplewise_center=True,\n",
    "                           rescale=1.0/255)   #samplewise_center:使输入数据的每个样本均值为0,rescale:归一化\n",
    " \n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'D:/image/dataset/image_train',\n",
    "    classes=['bhg','hg'],\n",
    "    target_size=(227,227),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    'D:/image/dataset/image_test1',\n",
    "    classes=['bhg','hg'],\n",
    "    target_size=(227,227),\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "checkpoint_filepath = './AlexNet_model3.h5'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "callback_learningrate = ReduceLROnPlateau(monitor='val_accuracy', mode='max', min_delta=0.03, patience=3, factor=.3, min_lr=0.000001, verbose=1)\n",
    "callbacks = [callback_checkpoint,callback_learningrate]\n",
    "\n",
    "'''开始训练'''\n",
    "model.fit_generator(generator=train_generator,steps_per_epoch=210/5,epochs=30,validation_data=validation_generator,validation_steps=47/5,callbacks=callbacks)\n",
    "\n",
    "yaml_string = model.to_yaml()  # 保存模型结构到yaml文件\n",
    "open('./model_architecture3.yaml', 'w').write(yaml_string)\n",
    "model.save_weights('./AlexNet_model3.h5')  #保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2aa3f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import cv2\n",
    "import urllib.request\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = model_from_yaml(open('./model_architecture3.yaml').read())\n",
    "model.load_weights('./AlexNet_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09675759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def url_to_image(url):\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    return image\n",
    "\n",
    "imgs = []\n",
    "image = url_to_image('https://img1.baidu.com/it/u=1138728463,1821870627&fm=26&fmt=auto&gp=0.jpg')\n",
    "# plt.imshow(image)\n",
    "img = cv2.resize(image,(256, 256))\n",
    "imgs.append(img)\n",
    "# plt.imshow(img)\n",
    "a = np.array(imgs)\n",
    "result=model.predict(a)\n",
    "idx = np.argmax(result)\n",
    "\n",
    "if idx == 0:\n",
    "    print('the image is no\\n')\n",
    "else:\n",
    "    print('the image is yes\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "adec6fa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-6be5392fab2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/image/微信图片_20210713161234.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m227\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m227\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\resize.cpp:4051: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "imgs=[]\n",
    "img=cv2.imread('D:/image/微信图片_20210713161234.jpg')\n",
    "img=cv2.resize(img,(227,227))\n",
    "imgs.append(img)\n",
    "a=np.array(imgs)\n",
    "result=model.predict(a)\n",
    "idx=np.argmax(result)\n",
    " \n",
    "if idx==0:\n",
    "    print('the image is hg\\n')\n",
    "else:\n",
    "    print('the image is bhg\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
